+++
date = "2015-09-30"
draft = false
weight = 07
title = "Lab 07 - Customer Instances"
+++


## Lab Objective

The objective of this lab is to teach basic administration of instances within the OpenStack cloud. Instances all have different 'states', and it is important to understand what those might be, and when it may be appropriate for an admin to force a customer instance into a different state. This lab has you select an instance from a project (which admin may or may not be a part of), and manipulate it as the OpenStack admin. If you finish this lab early, back through it again with an instance from a different project (tenant), or practice any admin skills involving instances (details / start / stop / suspend / pause / reboot / rebuild / resize / rescue).

#### 1. Getting at an instance with admin

1. SSH to your controller and log in as root

2. Once logged into the controller, issue the following command:

    ```	
    [root@controller ]# source keystonerc_admin
	```

3. First look at all of the instances in the on the OpenStack cloud. Note that the command we are about to issue will not tell us any details about the instances that are running (what compute node is hosting the instance, the flavor it is running, the image being utilized, and so on). Type the following command:

    ```
    [root@controller ~(keystone_admin)]# nova list --all-tenants
	+--------------------------------------+----------+----------------------------------+---------+------------+-------------+---------------------------------------+
    | ID                                   | Name     | Tenant ID                        | Status  | Task State | Power State | Networks                              |
    +--------------------------------------+----------+----------------------------------+---------+------------+-------------+---------------------------------------+
    | 07334a4c-dac3-474b-83f0-1d8f4db1f093 | vnc-test | 300b2cc45c3846939e589310ae714e46 | SHUTOFF | -          | Shutdown    | public=172.24.4.229; private=10.0.0.6 |
    | a06d7547-d380-44e6-9be8-650240faada0 | vt1      | ad1d9eeaf2884c7e8eac33ec0f3ef6e5 | ACTIVE  | -          | Running     | public=172.24.4.231; private=10.0.0.7 | <-- Our instance from the last lab!
    +--------------------------------------+----------+----------------------------------+---------+------------+-------------+---------------------------------------+
    ```

 * Your output might differ, after all this table is a reflection of what instances you have created. In the last lab, we all created an instance (vt1), so we should all see it up and running.
	
4. Highlight the id for the instance vt1 (NOT the Tenant ID), then issue the following command to get a list of details about the instance. Throughout this lab, we'll continually reference this highlighted ID, so we could keep highlighting it, or set a shell variable that we can reference.
 
 * We'll adhere to the convention of keeping all caps for the variable name (VAR_UUID), and set it to a simple string.

    ```
	[root@controller ~(keystone_admin)]# VAR_UUID=replace_with_the_id_for_vt1_you_highlighted
	[root@controller ~(keystone_admin)] # nova show $VAR_UUID
	```

5. Answer the following questions about the output:

 * What time was this instance created?
 * What flavor is being utilized?
 * Is the machine powered on, or powered off?
 * Which compute node is hosting this instance?
 * What is the IP address of this instance?
 * To what tenant does this instance belong?
 * Hint: Having trouble figuring out the tenant to which this instance belongs? Here's what to do, locate the value for 'tenant_id', it is a long UUID value. Issue a request to the identity service (keystone) for a list of all of the tenant IDs.

    ```
	[root@controller ~(keystone_admin)] # keystone tenant-list
    ```
	
6. To reveal the name of the tenant, match the value of 'tenant_id' with the displayed ids. Sometimes administrators need to troubleshoot issues with an instance. An admin can retrieve diagnostic information about an instance with the following command (once again, highlight the id of the instance)

    ```
	[root@controller ~(keystone_admin)] # nova diagnostics $VAR_UUID
    ```
	
7. Answer the following questions about the output from nova diagnostics:

 * (CPU details) What is the CPU time (in nano seconds)?
 * (Memory details) What is the max amount of memory (in MB)?
 * (Memory details) What is the actual amount of memory currently being used?
 * (Network details) How many octets have been received?
 * (Network details) How many octets have been dropped?
 * (Network details) How many packets have been transmitted?
 * (Disk details) How many disk reads have their been (in bytes)?
 * (Disk details) How many disk write requests have their been?

8. Now let's set some metadata about our instance. Here I am using mtag1, but you can use anything. Try starting with the following:

    ```
	[root@controller ~(keystone_admin)]# nova meta $VAR_UUID set mtag1='jumper box'
    ```
	
9. Look to see that your first metadata tag has been set.

    ```
	[root@controller ~(keystone_admin)] # nova show $VAR_UUID
    ```
	
10. Now let's set a second metadata tag concerning our instance.

	`[root@controller ~(keystone_admin)] # nova meta $VAR_UUID set mtag2='uh oh I set this tag incorrectly'`

0. Look to see that your second metadata tag has been set.

	`[root@controller ~(keystone_admin)] # nova show $VAR_UUID`

0. Now remove the second metadata tag.

	`[root@controller ~(keystone_admin)] # nova meta $VAR_UUID delete mtag2`

0. Confirm the second metadata tag has been deleted.

	`[root@controller ~(keystone_admin)] # nova show $VAR_UUID`

#### 2. Starting and Stopping instances with admin

In this section, we'll start and stop an instance. If an instance is already stopped, it can't hurt to start it. Just like if it's started, if won't hurt it to start the instance 'again' (in both cases a 409 HTTP error is returned).

0. Make sure the instance is stopped.

	`[root@controller ~(keystone_admin)] # nova stop $VAR_UUID`

0. Check to see if the instance is really stopped.

	`[root@controller ~(keystone_admin)] # nova show $VAR_UUID`

0. Start the instance. 
	
	> Note that admin may or may not be part of this project, and is still able to manipulate this instance.

	`[root@controller ~(keystone_admin)] # nova start $VAR_UUID`

0. Is the instance running? Find out by using the nova show command.

	`[root@controller ~(keystone_admin)] # nova show`
 
#### 3. Pausing and unpausing instances with admin

In this section we'll pause and unpause a running instance. Pausing an instance stores the instance in memory (RAM), so in laptop terms, think of it as putting the instance into "sleep" mode.

0. Try pausing the instance.

	`[root@controller ~(keystone_admin)] # nova pause $VAR_UUID`

0. Use the CLI to confirm that the instance is paused

	`[root@controller ~(keystone_admin)] # nova show`

0. Use the browser to confirm that the instance is paused. Log into the Horizon OpenStack dashboard as `chestercopperpot:fa5tpa55w0rd` and confirm that the instance is indeed paused.

0. Try unpausing the instance

	`[root@controller ~(keystone_admin)] # nova unpause $VAR_UUID`

0. Use the CLI to confirm that the instance is unpaused

	`[root@controller ~(keystone_admin)] # nova show`

0. Once again, use the Horizon OpenStack dashboard as `chestercopperpot:fa5tpa55w0rd` to confirm that the instance is unpaused.

#### 4. Suspending and resuming instances with admin

In this section we'll suspend and resume a running instance. Suspending an instance stores the instance on the disk, so in laptop terms, think of it as putting the instance into "hibernation" mode.

0. Try suspending the instance

	`[root@controller ~(keystone_admin)] # nova suspend $VAR_UUID`

0. Use the CLI to confirm that the instance is suspended

	`[root@controller ~(keystone_admin)] # nova show`

0. Use the browser to confirm that the instance is suspended. Log into the Horizon OpenStack dashboard as `chestercopperpot:fa5tpa55w0rd` and confirm that the instance is indeed suspended.

0. Try resuming the instance

	`[root@controller ~(keystone_admin)] # nova resume $VAR_UUID`

0. Use the CLI to confirm that the instance is resumed.

	`[root@controller ~(keystone_admin)] # nova show`

0. Once again, use the Horizon OpenStack dashboard as 'chestercopperpot:fa5tpa55w0rd' to confirm that the instance is resumed.


















#### 4. Log into the newly created instance via the CLI

The objective of this section is to log into the newly created instance from the CLI. In order to do so, we'll need to work with namespaces which will be known to the Neutron and compute nodes. Therefore, we'll start by logging into the Neutron server and exploring the known namespaces.

1. SSH to your Neutron node and log in as root. Do this by tying the following:

    ```
	root@controller ~(keystone_chestercopperpot)]# ssh root@neutron
	```

2. Display the known namespaces. These are typically ordered in the way they were created (most recent on the bottom). We are concerned with those that begin with 'qrouter-', so you could assume that the bottom most namespace is the one we need.

    ```
    [root@neutron ~]# ip netns list
	[root@neutron ~]# ip netns list
    qrouter-c7020d9b-7916-4674-b3fb-236be63b9012
    qdhcp-20ca30a2-e3fb-4e62-bd78-08dc471e93ed

    ```
	
3. The resulting list should look something like the screenshot below. Notice, there are two qrouter entries. One is for the router associated with acme_inc, and the second is for the router associated with vault_tek (the one we want). To determine which namespace ID we need, we'll have to go on a little journey. There's actually two fairly simply methods to determine which one is correct; one uses the CLI, and the other uses Horizon. Try both of the following methods to determine which one is correct.
nova 
    ![](https://i.imgur.com/8G9qvhe.png)

0. The CLI way of doing things is to create a keystonerc_chestercopperpot file on the neutron server that we can source.

    `[root@neutron ] # nano keystonerc_chestercopperpot`

0. Copy and paste the following text into the new file `keystonerc_chestercopperpot`

    ```
    export OS_USERNAME=chestercopperpot
    export OS_TENANT_NAME=vault_tek
    export OS_PASSWORD=fa5tpa55w0rd
    export OS_AUTH_URL=http://192.168.0.10:5000/v2.0/
    export OS_REGION_NAME=RegionOne
    export PS1='[\u@\h \W(keystone_chestercopperpot)]\$ '
    ```

    `[root@neutron ] # source keystonerc_chestercopperpot`

    `(keystone_chestercopperpot)# neutron rouhelp | less
	ter-list`

0. What is displayed should be a list of all the routers associated with the project for which chestercopperpot is associated (vault_tek). Notice that the id with the red circle matches the bottom most qrouter entry that was displayed when we ran the `ip netns list` command. This is our indication that we're looking at the correct namespace id. However, we can further verify this with Horizon.

    ![](https://i.imgur.com/tZcv1lD.png)

0. Log into the Horizon dashboard.

### 5. Stop & remove an instance via the CLI

1. We'll continue to explore manipulating instances, but for now issue the following commands to stop, and then remove the instance (vt1):

    ```
    [root@controller ~(keystone_chestercopperpot)]# nova stop vt1
	## You'll need to wait until the server stops before you can issue the next command. If it fails, just wait longer.
    [root@controller ~(keystone_chestercopperpot)]# nova force-delete vt1
	```
	
2. The instance has now been removed. Confirm with the following command:
 
    ```
	[root@controller ~(keystone_chestercopperpot)]# nova list
	```

